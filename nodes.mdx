---
title: Node Basics
sidebar_label: Node Basics
sidebar_position: 2
slug: /concepts/workflow-orchestration/nodes
---

# Comprehensive Guide on Building a Workflow in the Platform

Welcome to the comprehensive guide on building a workflow in the platform using different nodes to add datasets, perform operations on datasets, and persist and view results with a precondition of the previous two scenarios.

**Modules Covered**: Nodes on workflow, Navigation and Linking of Nodes, Deploy and Run to persist the results.

## What is a Node?

A node in a platform UI context, especially in data processing and analytics platforms, serves as building blocks in a workflow to represent an operation, transformation, or represent a dataset. Nodes can be interconnected to form a pipeline or workflow that processes data step-by-step.

Here is how a simple workflow looks like. By linking different nodes like the Dataset Reader, transform nodes (Select Operation), a data sink node is generated by performing the corresponding transformation onto the dataset.

![Simple Workflow](https://supportsite.azureedge.net/supportsite/image-20240522-182118.png)

## Types of Nodes Available

### 1. Dataset Reader

A “Dataset Reader” is a graphical representation of a dataset or data source within a workflow in the platform. It typically appears as a graphical element that we can interact with to incorporate data into the workflows.

**Use Case**: Dataset Readers serve as a point of entry and allow us to import, access, and manipulate data uploaded on the platform as part of a larger data processing or analysis task.

### 2. Transform Node

A "transform node" in the context of a platform UI is a visual component within a workflow used to perform data transformation operations, including SQL operations, as part of a data processing or analytics workflow. Transform nodes play a crucial role in data processing workflows by facilitating the manipulation and restructuring of data.

**Use Case**: It allows us to apply various transformations to their data, including SQL operations such as filtering, joining, partitioning, grouping, aggregating, sorting, union or selecting, and dropping specific columns.

### 3. Custom Node

A custom node in the context of a platform UI is a versatile and configurable component that allows us to add their own code, push updates to version control systems like Git, and perform any operation on datasets linked to it from previous nodes. This node provides a flexible environment for executing custom logic and integrating it into the broader data processing workflow.

**Use Case**: One can create their own code repositories, commit changes, and manage branches directly from the custom node interface and make it possible to manipulate datasets on the workflow.

## Navigation - How to Add and Use a Node in the Workflow?

To add or create a node on the workflow, click on the ">" button. This will expand a panel with all nodes that can be used.

![Navigation Panel](https://supportsite.azureedge.net/supportsite/image-20240522-194733.png)

![Node Creation](https://supportsite.azureedge.net/supportsite/image-20240522-195743.png)

To create a new node, i.e., a Custom node, Click on “Add Node” (pointed in green), which prompts you to a small window to name your custom node and define its type. After which, its corresponding node environment opens in the same window and workflow that you are currently using.

After creating custom configurable nodes and such, to get them updated on the panel, Click on the Refresh button near “Nodes” (pointed in red).

To check for nodes available under every Node Type, similarly, Click on the “v” button to expand any of the necessary node types that you need to use. It will open and list down all nodes existing under that type to be added to the workflow.

Once decided, Click, Drag, and Drop the Node onto the workflow that you are currently using.

![Node Workflow](https://supportsite.azureedge.net/supportsite/image-20240522-201144.png)

The Warning symbol over the node appears until the necessary fields on the node are filled and saved.

**Example**: Dataset Reader - The node can be renamed. A dataset has to be added to the node and saved.

### Naming Conventions for Any Node in the Workflow

Make sure the name does not contain any white spaces (" "), special characters ('!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '=', '+', '\', '|', '[', ']', '{', '}', ':', '/', '?', '.', '>', ','), and the length of characters is as short as possible.

To access a node and configure it as per our needs, Click on the node which directs you to a panel on the right end of the window. Here, there are two sections:

- **Metadata**
  - Name of Node - Rename it based on Naming Rules for different Node Type
  - Description of Node - Provide brief context on the functionality for user understanding
- **Configuration** - differs based on Node Types

The node panel offers two usage modes:

- **UI Mode**: Nodes' names, descriptions, and configurations are displayed as dropdown box fields for straightforward interaction. Here, all the available fields can be viewed to avoid any typos.
- **JSON Mode**: Nodes' configurations are represented as editable key-value pairs in a JSON format, allowing for greater flexibility. One should already be aware of the operations and fields present to modify the config code as per needs.

**Example**: Sink Data Node

**UI Mode**:

![UI Mode](https://supportsite.azureedge.net/supportsite/image-20240523-062258.png)

**JSON Mode**:

![JSON Mode](https://supportsite.azureedge.net/supportsite/image-20240523-062227.png)

## How to Link the Nodes in a Workflow and Run the Process?

To link two nodes in a workflow, there are certain pre-conditions to be checked for the deployment to be successful.

**Example**:

- Transform node - Needs a predecessor Dataset Reader node to support the operations to be performed.
- Custom node - Needs a compute dataset node to support operations and access the dataset from S3 / RedShift / any source.

Now, to link two nodes, Click on the dot on the right/bottom end of one node, Drag and Drop it to the dot on the left/upper end of the other node to establish a connection between them.

![Link Nodes](https://supportsite.azureedge.net/supportsite/image-20240522-205711.png)

![Link Nodes](https://supportsite.azureedge.net/supportsite/image-20240522-205515.png)

Only after performing this, for say, Connecting a Dataset Reader Node to a Transform node, can one use the fields/columns on the dataset used, to perform the operation and create the resultant data sink node.

### To View the Sample Data on Any Sink Node:

Click on the Run button on the generated Data Sink Node and wait for it to complete. Once completed, we can see that the nodes have a green correct sign on top of it to verify that the transformation was performed successfully. If not, a red cross sign would pop up and you can check the Console on the left bottom corner to view the error occurred.

![Run Node](https://supportsite.azureedge.net/supportsite/image-20240523-151525.png)

After running the Data Sink Node, Click on the Info button on the node, which opens the Sample Data panel on the Right-pane where you can view different kinds of information about the data, type of data values generated, and Sample data with top 20 rows of the resultant output.

![Sample Data](https://supportsite.azureedge.net/supportsite/image-20240523-151613.png)

To learn in depth about every type of node and its specific use cases:
- [Dataset Nodes](#)
- [Transform Nodes](#)
- [Custom Nodes](#)

Voila! Now you have created a workflow by connecting apt nodes to accomplish the task. But it doesn’t end here!

## To Persist / Visualize the Resultant Sink Data:

The workflow has to be deployed without failures and run to create a job for the workflow.

### Steps Before Running the Workflow:

- Checks to be made: Click on every Node used & Change the Names as required (Make sure there are no special characters in the name).
- Click on the Sink Nodes generated and make sure the Persist checkbox is checked. This is done to create a dataset that can be viewed in the Dataset Listing Page on Platform.
- Save your Workflow & Deploy it. Once deployed - Run the workflow.

![Deploy Workflow](https://supportsite.azureedge.net/supportsite/image-20240523-064058.png)

![Deploy Workflow](https://supportsite.azureedge.net/supportsite/image-20240523-064119.png)

This will redirect to the Job created for the Workflow run, view the Job to witness the successful run of your creation:

![Job Creation](https://supportsite.azureedge.net/supportsite/image-20240502-064728.png)

If there are any nodes not running successfully, we can use the Console available in the Jobs screen in the bottom left to check why the Workflow does not work.

Once the run is complete - The datasets would be created. To view the datasets created, navigate to:

**Data Hub → Document Manager → Datasets**

Based on the time of creation, filter and navigate your generated dataset after which, you can start visualizing them using Metrics Visualizer and also automate Reports from them!

To learn How to visualize your data using the Metrics Visualizer, and understand its features and functionalities, Refer to this Confluence page: [Metrics Visualizer: Unveiling Insights](#)

## Example Workflow

Below is an Example Workflow created using various operations on the Quince dataset to get the Sum of Clicks, on a date level, for the filtered Date Range.

![Example Workflow](https://supportsite.azureedge.net/supportsite/image-20240523-153813.png)

We have successfully learned the basics of using any Node in a Workflow, Deployment, running a Job, and the ideology/use case behind this work!